# Model Training 

The model is a pretrained LLaMA 3.2 1B, which is fine-tuned using LoRA (Low-Rank Adaptation) on the MedMCQA dataset.

## Results 


### Training Run - 1 

![Output](../.assets/04/01_Output.png)


Training Time : ~25 Hours


### Training Run - 2

![Output](../.assets/04/02_Output.png)

mps out of memory at epoch = 2, batch = 2 gradient =16/8
# What's a LLM 

A LLM (Large Language Model) is a type of AI model trained on massive amounts of text data to understand and generate human-like language. Examples include OpenAI’s GPT-4, Meta’s LLaMA, and Google’s Gemini.

LLMs are built using Transformer architectures, a deep learning model introduced in the paper "Attention Is All You Need" (2017). One of the most well-known Transformer-based architectures is GPT (Generative Pre-trained Transformer), which uses a self-attention mechanism to process text efficiently. Unlike older models like RNNs (Recurrent Neural Networks) and LSTMs (Long Short Term Memory), Transformers can handle long-range dependencies, enabling LLMs to generate coherent and context-aware text.

Read more about transformers in this paper, which introduced the architecture that led to the creation of LLMs - [Attention Is All You Need](https://arxiv.org/pdf/1706.03762)

**Example Use Case:**

User: "What is photosynthesis?"

LLM Response: "Photosynthesis is the process by which plants convert sunlight into energy."

## Fine-Tuning LLM 

Fine-tuning is the process of training a pre-trained model on a smaller, domain-specific dataset to improve its performance for a specific task. It adjusts the model’s weights while retaining general knowledge from its original training.

